import numpy as np

def gradient_descent(learning_rate=0.1, initial_guess=(0, 0), num_iterations=1000, tolerance=1e-6):
    # Define the function
    def F(x, y):
        return x**2 + y**2 - x*y + x - y + 1
    
    # Compute the gradient of the function
    def gradient(x, y):
        df_dx = 2*x - y + 1
        df_dy = 2*y - x - 1
        return np.array([df_dx, df_dy])
    
    # Initialize variables
    x, y = initial_guess
    for _ in range(num_iterations):
        grad = gradient(x, y)
        new_x = x - learning_rate * grad[0]
        new_y = y - learning_rate * grad[1]
        
        # Check for convergence
        if np.linalg.norm(np.array([new_x - x, new_y - y])) < tolerance:
            break
        
        x, y = new_x, new_y
    
    return x, y, F(x, y)

# Example usage:
x_min, y_min, f_min = gradient_descent()
print("Minimum point (x, y):", (x_min, y_min))
print("Function value at minimum point:", f_min)
